{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ublogo.png\"/>\n",
    "\n",
    "### CSE610 - Bayesian Non-parametric Machine Learning\n",
    "\n",
    "  - Lecture Notes\n",
    "  - Instructor - Varun Chandola\n",
    "  - Term - Fall 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "The objective of this notebook is to provide detailed discussions about using Gaussian Processes (GP) for classification problems.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** This material is based on Chapter 3 of the GPML book.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn out attention to *classification* problems, where the target is not real, but categorical, i.e., $y \\in \\{C_1,C_2,\\ldots,C_\\mathcal{C}\\}$, where $\\mathcal{C}$ is the number of possible classes that the target can be in. We will work with probabilistic classification, where we are interested in:\n",
    "$$\n",
    "P(Y = C_1\\vert X = {\\bf x})\n",
    "$$\n",
    "> There are two general approaches to building probabilistic classifiers. One is to start with the joint probability distribution, $p({\\bf x},y)$, which can be decomposed as $p({\\bf x}\\vert y)p(y)$ and then use *Bayes rule* to get $p(y\\vert{\\bf x})$. The other is to directly model $p(y\\vert {\\bf x})$ without making any assumptions about $p({\\bf x})$. The former approach is called **generative models** and the latter approach is called **discriminative models**. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Gaussian process classification is discriminative.\n",
    "</div>\n",
    "\n",
    "We will see that, to a certain extent, applying GP to probabilistic classification is similar to applying GP to regression, as we saw earlier. However, the problem is much more demanding due to one key reason: *the likelihood function in a classification setting cannot be Gaussian*. This leads to a challenging issue that requires approximate solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear logistic regression model\n",
    "Consider a binary classification setting, i.e., the targets can either be $+1$ or $-1$. In the logistic regression model, we directly model the conditional probability for the targets, as follows:\n",
    "$$\n",
    "p(y = +1 \\vert {\\bf x},{\\bf w}) = \\sigma({\\bf w}^\\top{\\bf x})\n",
    "$$\n",
    "> $\\sigma(\\cdot)$ can be any *sigmoid* function, which is any monotonically increasing function from $\\mathbb{R} \\rightarrow [0,1]$. \n",
    "\n",
    "> A popular choice for $\\sigma(\\cdot)$ is the *logistic response function*, such that $\\sigma(z) = \\frac{1}{1 + \\exp{-z}}$, in which case the classification model is called **linear logistic regression** (or just **logistic regression**).\n",
    "\n",
    "> However, there are other possible choices as well. For instance, the **linear probit regression** model uses the probit function, which is the cumulative density of a standard normal distribution, i.e., $\\sigma(z) = \\int_{-\\infty}^z\\mathcal{N}(u\\vert 0,1)du$\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Note:** For the subsequent discussions, we will use the logistic function as the response function.\n",
    "</div>\n",
    "\n",
    "Using this model, we can get the probability of $y$ to be any one of the labels:\n",
    "$$\n",
    "p(y = +1 \\vert {\\bf x},{\\bf w}) = \\sigma({\\bf w}^\\top{\\bf x})\\text{;   } p(y = -1 \\vert {\\bf x},{\\bf w}) = 1 - p(y = +1 \\vert {\\bf x},{\\bf w}) = \\sigma(-{\\bf w}^\\top{\\bf x})\n",
    "$$\n",
    "The above result uses the fact that the logistic function is symmetric, i.e., $1 - \\sigma(z) = \\sigma(-z)$.\n",
    "\n",
    "Thus, if we are given a training data set, $\\mathcal{D} = \\{({\\bf x}_i,y_i)\\vert i = 1,\\ldots,N\\}$, the probability of the $i^{th}$ label is given by:\n",
    "$$\n",
    "p(y_i\\vert {\\bf x}_i,{\\bf w}) = \\sigma(y_i{\\bf w}^\\top{\\bf x}_i)\n",
    "$$\n",
    "\n",
    "#### The non-Bayesian formulation of logistic regression\n",
    "To learn the optimal weights, ${\\bf w}$, one can calculate the log-likelihood of the training data set, i.e., \n",
    "$$\n",
    "\\mathcal{L}(\\mathcal{D}\\vert {\\bf w}) = \\sum_{i=1}^N \\log{\\sigma(y_i){\\bf w}^\\top{\\bf x}_i}\n",
    "$$\n",
    "and then find the ${\\bf w}$ that maximizes the log-likelihood. This can be done using a gradient based optimizer.\n",
    "\n",
    "#### The Bayesian formulation of logistic regression\n",
    "Assuming a Gaussian prior on ${\\bf w}$, i.e., ${\\bf w} \\sim \\mathcal{N}({\\bf 0},\\Sigma_p)$, the posterior distribution for ${\\bf w}$ will be:\n",
    "$$\n",
    "p({\\bf w}\\vert \\mathcal{D}) = \\frac{p(\\mathcal{D}\\vert{\\bf w})p({\\bf w})}{\\int p(\\mathcal{D}\\vert{\\bf w}')p({\\bf w}')d{\\bf w}'}\n",
    "$$\n",
    "\n",
    "Now, this is the start of all our troubles :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
