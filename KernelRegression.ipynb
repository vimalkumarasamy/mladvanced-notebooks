{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ublogo.png\"/>\n",
    "\n",
    "### CSE610 - Bayesian Non-parametric Machine Learning\n",
    "\n",
    "  - Lecture Notes\n",
    "  - Instructor - Varun Chandola\n",
    "  - Term - Fall 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "The objective of this notebook is to provide detailed discussions about non-linear regression using Kernel methods.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** This material is based on Chapter 2 of the GPML book.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making regression more expressive\n",
    "One issue with Linear Regression is that it can only learn linear relationship between the inputs and the target. One simple idea to overcome this issue is to **project** the inputs into a high-dimensional space, using a set of non-linear basis functions, and then apply the linear model in this new space.\n",
    "\n",
    "Let $\\phi({\\bf x})$ be the function that maps ${\\bf x}$ from $\\mathbb{R}^D$ to $\\mathbb{R}^P$. The regression model will now become:\n",
    "$$\n",
    "f({\\bf x}) = {\\bf w}^\\top{\\phi(\\bf x})\\text{;     }  y = f({\\bf x}) + \\epsilon\n",
    "$$\n",
    "\n",
    "Many of the derivation steps are similar to Bayesian regression, so will be skipped here. The final predictive distribution can be written as:\n",
    "\\begin{equation}\n",
    "p(f_*\\vert {\\bf x}_*,X,{\\bf y}) = \\mathcal{N}\\left(\\frac{1}{\\sigma_n^2}{\\bf \\phi}_*^\\top A^{-1}{\\bf \\Phi}{\\bf y},{\\bf \\phi}_*^\\top A^{-1}{\\bf \\phi}_*\\right)\n",
    "\\end{equation}\n",
    "where $\\phi_*$ is a shorthand for $\\phi({\\bf x}_*)$, ${\\bf \\Phi}$ is a $P \\times N$ matrix such that the $i^{th}$ column corresponds to $\\phi({\\bf x}_i)$, and $A= \\sigma_n^{-2}\\Phi\\Phi^\\top + \\Sigma_p^{-1}$. This equation can be rewritten as:\n",
    "\\begin{equation}\n",
    "p(f_*\\vert {\\bf x}_*,X,{\\bf y}) = \\mathcal{N}\\left(\\phi_*^\\top\\Sigma_p\\Phi(K + \\sigma_n^2I)^{-1}{\\bf y},\\phi_*^\\top\\Sigma_p\\phi_* - \\phi_*^\\top\\Sigma_p\\Phi(K + \\sigma_n^2I)^{-1}\\Phi^\\top\\Sigma_p{\\bf \\phi}_*\\right)\n",
    "\\end{equation}\n",
    "where the $K$ is a $N \\times N$ matrix, defined as $K = \\Phi^\\top\\Sigma_p\\Phi$.\n",
    "\n",
    "> How? Homework 1 problem 1.\n",
    "\n",
    "> Why? See below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the above equation, the feature space (${\\bf x}$) is always within one of the following three entities:\n",
    "- ${\\bf \\Phi}^\\top\\Sigma_p{\\bf \\Phi}$\n",
    "- $\\phi_*^\\top\\Sigma_p{\\bf \\Phi}$\n",
    "- $\\phi_*^\\top\\Sigma_p\\phi_*$\n",
    "\n",
    "### Introducing a kernel (covariance) function\n",
    "Consider the function, $k({\\bf x},{\\bf x}') = \\phi({\\bf x})^\\top\\Sigma_p\\phi({\\bf x})$, where ${\\bf x}$ and ${\\bf x}'$ are two data instances in the training or test data set.\n",
    "\n",
    "> $k(\\cdot,\\cdot)$ is referred to as a *covariance* or *kernel* function\n",
    "\n",
    "But typically kernel methods are applicable where a data instance occurs only in a dot-product with other data instance, i.e., ${\\bf x}^\\top{\\bf x}'$.\n",
    "\n",
    "Consider the following scalar quantity: $\\phi({\\bf x})^\\top\\Sigma_p\\phi({\\bf x})$. We know that $\\Sigma_p$ is positive definite or *pd* (covariance matrix of a multivariate Gaussian distribution has to be positive-definite, otherwise it will not be invertible). \n",
    "\n",
    "Since $\\Sigma_p$ is *pd* and symmetric, its square root exists, i.e., \n",
    "$$\n",
    "\\Sigma_p = (\\Sigma_p^{1/2})^2\n",
    "$$\n",
    "Which means that if $\\psi({\\bf x}) = \\Sigma_p^{1/2}\\phi({\\bf x})$, then:\n",
    "$$\n",
    "k({\\bf x},{\\bf x}') = \\psi({\\bf x})^\\top\\psi({\\bf x})\n",
    "$$\n",
    "Which means that one can now employ the *kernel trick* to convert Bayesian linear regression to Bayesian kernel regression, by replacing the above $k(\\cdot,\\cdot)$ with any suitable kernel function.\n",
    "\n",
    "### Applying the kernel trick\n",
    "The modified predictive distribution can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "p(f_*\\vert {\\bf x}_*,X,{\\bf y}) = \\mathcal{N}\\left({\\bf k}_*^\\top(K + \\sigma_n^2I)^{-1}{\\bf y},k({\\bf x}_*,{\\bf x}_*) - {\\bf k}_*^\\top(K + \\sigma_n^2I)^{-1}{\\bf k}_*\\right)\n",
    "\\end{equation}\n",
    "where $K$ is a (kernel/covariance) matrix calculated from the input training data set, $X$, such that $K[i,j] = k({\\bf x}_i,{\\bf x}_j)$, and ${\\bf k}_*$ is a $N \\times 1$ vector such that ${\\bf k}[i] = k({\\bf x}_i,{\\bf x}_*)$.\n",
    "\n",
    "We will see later that Gaussian Process Regression has the same formulation as above, thus GPR can be treated as Bayesian Kernel Regression. However, we will study GPR from a different perspective."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
